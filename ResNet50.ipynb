{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b253bd-a3b8-4ec0-b0c0-a5a46e161634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\chhama\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chhama\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\chhama\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\chhama\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chhama\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chhama\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5be299a8-8b3a-4b1d-a089-549aa7c09abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\chhama\\anaconda3\\envs\\tf_env\\lib\\site-packages (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9dea598-b969-4f19-ab92-a4fbc58b3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import random \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61071453-9e0b-453e-954c-16ad3d72acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install pillow\n",
    "    from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7115f79-62cb-4e07-a07e-9a9a0f79fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset path \n",
    "base_path = r'C:\\Users\\Chhama\\OneDrive\\Desktop\\Banana'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8178f1-d036-484d-bcf0-df34f65c27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize all images \n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5632679b-08e5-4ab8-9212-b1af0540227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing and loading function\n",
    "def load_and_balance_dataset(folder_path, target_count=None):\n",
    "    X, y = [], []\n",
    "    class_data = {}\n",
    "\n",
    "    # First pass to collect and resize images\n",
    "    for folder in os.listdir(folder_path):\n",
    "        if not folder.startswith('Banana'):\n",
    "            continue\n",
    "        try:\n",
    "            range_part = folder.split('(')[1].split(')')[0]\n",
    "            low, high = map(int, range_part.split('-'))\n",
    "            avg_shelf_life = (low + high) / 2.0\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        folder_full_path = os.path.join(folder_path, folder)\n",
    "        class_images = []\n",
    "\n",
    "        for img_file in os.listdir(folder_full_path):\n",
    "            img_path = os.path.join(folder_full_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, IMG_SIZE)\n",
    "                img = img / 255.0\n",
    "                class_images.append(img)\n",
    "\n",
    "        class_data[folder] = (class_images, avg_shelf_life)\n",
    "\n",
    "    # Determine max class size\n",
    "    if not target_count:\n",
    "        target_count = max(len(images) for images, _ in class_data.values())\n",
    "\n",
    "    augmentor = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "        # brightness_range disabled\n",
    "    )\n",
    "\n",
    "    # Apply balancing with augmentation\n",
    "    for folder, (images, shelf_life) in class_data.items():\n",
    "        current_count = len(images)\n",
    "        if current_count < target_count:\n",
    "            to_add = target_count - current_count\n",
    "            print(f\"Augmenting class '{folder}' with {to_add} images...\")\n",
    "\n",
    "            for _ in range(to_add):\n",
    "                img = random.choice(images)\n",
    "                img_aug = augmentor.random_transform(img)\n",
    "                X.append(img_aug)\n",
    "                y.append(shelf_life)\n",
    "\n",
    "        # Add original images\n",
    "        X.extend(images)\n",
    "        y.extend([shelf_life] * len(images))\n",
    "\n",
    "    print(\"Final class distribution after balancing:\")\n",
    "    for folder, (images, _) in class_data.items():\n",
    "        print(f\"{folder}: {target_count}\")\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a12934-0ec1-4a6b-b05e-d8fb65973fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'Banana(1-5)' with 102 images...\n",
      "Augmenting class 'Banana(10-15)' with 100 images...\n",
      "Augmenting class 'Banana(5-10)' with 69 images...\n",
      "Final class distribution after balancing:\n",
      "Banana(1-5): 180\n",
      "Banana(10-15): 180\n",
      "Banana(15-20): 180\n",
      "Banana(5-10): 180\n"
     ]
    }
   ],
   "source": [
    "# Load and balance dataset\n",
    "X, y = load_and_balance_dataset(base_path)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb827c02-5436-4816-9de7-53629874e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ResNet50-based regression model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = \"best_banana_model.h5\"\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
    "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a1c33f8-7b42-4180-b289-0645923d1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "18/18 [==============================] - 95s 5s/step - loss: 4.2088 - mae: 4.6740 - val_loss: 10.1171 - val_mae: 10.6171 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "18/18 [==============================] - 77s 4s/step - loss: 2.1495 - mae: 2.6111 - val_loss: 9.8516 - val_mae: 10.3516 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "18/18 [==============================] - 77s 4s/step - loss: 1.7733 - mae: 2.2243 - val_loss: 9.7198 - val_mae: 10.2198 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "18/18 [==============================] - 74s 4s/step - loss: 1.6526 - mae: 2.1060 - val_loss: 9.4880 - val_mae: 9.9880 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "18/18 [==============================] - 73s 4s/step - loss: 1.5101 - mae: 1.9459 - val_loss: 9.2409 - val_mae: 9.7409 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "18/18 [==============================] - 74s 4s/step - loss: 1.6498 - mae: 2.1033 - val_loss: 8.8420 - val_mae: 9.3420 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.3031 - mae: 1.7409 - val_loss: 8.4869 - val_mae: 8.9869 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.5533 - mae: 1.9945 - val_loss: 8.2812 - val_mae: 8.7812 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.3382 - mae: 1.7774 - val_loss: 8.0645 - val_mae: 8.5645 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      "18/18 [==============================] - 70s 4s/step - loss: 1.2195 - mae: 1.6502 - val_loss: 7.8891 - val_mae: 8.3891 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "18/18 [==============================] - 79s 4s/step - loss: 1.1983 - mae: 1.6267 - val_loss: 7.6459 - val_mae: 8.1459 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "18/18 [==============================] - 92s 5s/step - loss: 1.0893 - mae: 1.5047 - val_loss: 7.2296 - val_mae: 7.7206 - lr: 1.0000e-04\n",
      "Epoch 13/40\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.1242 - mae: 1.5527 - val_loss: 6.8603 - val_mae: 7.3305 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3750 - mae: 1.8191 - val_loss: 6.2953 - val_mae: 6.7459 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "18/18 [==============================] - 91s 5s/step - loss: 1.2149 - mae: 1.6485 - val_loss: 5.7986 - val_mae: 6.2600 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1036 - mae: 1.5287 - val_loss: 5.0880 - val_mae: 5.5503 - lr: 1.0000e-04\n",
      "Epoch 17/40\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.0920 - mae: 1.5176 - val_loss: 4.1615 - val_mae: 4.6309 - lr: 1.0000e-04\n",
      "Epoch 18/40\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.1834 - mae: 1.6128 - val_loss: 4.1178 - val_mae: 4.5756 - lr: 1.0000e-04\n",
      "Epoch 19/40\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0657 - mae: 1.4920 - val_loss: 2.9683 - val_mae: 3.4315 - lr: 1.0000e-04\n",
      "Epoch 20/40\n",
      "18/18 [==============================] - 87s 5s/step - loss: 1.0846 - mae: 1.5205 - val_loss: 3.0754 - val_mae: 3.5292 - lr: 1.0000e-04\n",
      "Epoch 21/40\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.9401 - mae: 1.3551 - val_loss: 2.6926 - val_mae: 3.1432 - lr: 1.0000e-04\n",
      "Epoch 22/40\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1118 - mae: 1.5447 - val_loss: 2.2567 - val_mae: 2.7158 - lr: 1.0000e-04\n",
      "Epoch 23/40\n",
      "18/18 [==============================] - 90s 5s/step - loss: 1.1152 - mae: 1.5473 - val_loss: 1.7068 - val_mae: 2.1570 - lr: 1.0000e-04\n",
      "Epoch 24/40\n",
      "18/18 [==============================] - 95s 5s/step - loss: 1.1757 - mae: 1.6110 - val_loss: 2.2839 - val_mae: 2.7601 - lr: 1.0000e-04\n",
      "Epoch 25/40\n",
      "18/18 [==============================] - 98s 6s/step - loss: 0.9426 - mae: 1.3537 - val_loss: 1.6209 - val_mae: 2.0782 - lr: 1.0000e-04\n",
      "Epoch 26/40\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.9778 - mae: 1.4015 - val_loss: 1.5241 - val_mae: 1.9720 - lr: 1.0000e-04\n",
      "Epoch 27/40\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.9515 - mae: 1.3761 - val_loss: 1.5275 - val_mae: 1.9895 - lr: 1.0000e-04\n",
      "Epoch 28/40\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0751 - mae: 1.5005 - val_loss: 1.2508 - val_mae: 1.6891 - lr: 1.0000e-04\n",
      "Epoch 29/40\n",
      "18/18 [==============================] - 90s 5s/step - loss: 0.9592 - mae: 1.3847 - val_loss: 1.2663 - val_mae: 1.7082 - lr: 1.0000e-04\n",
      "Epoch 30/40\n",
      "18/18 [==============================] - 88s 5s/step - loss: 1.0068 - mae: 1.4336 - val_loss: 1.3513 - val_mae: 1.7793 - lr: 1.0000e-04\n",
      "Epoch 31/40\n",
      "18/18 [==============================] - 90s 5s/step - loss: 0.9549 - mae: 1.3720 - val_loss: 1.1226 - val_mae: 1.5828 - lr: 1.0000e-04\n",
      "Epoch 32/40\n",
      "18/18 [==============================] - 91s 5s/step - loss: 0.9465 - mae: 1.3658 - val_loss: 1.1394 - val_mae: 1.5851 - lr: 1.0000e-04\n",
      "Epoch 33/40\n",
      "18/18 [==============================] - 92s 5s/step - loss: 0.9312 - mae: 1.3508 - val_loss: 0.9729 - val_mae: 1.3986 - lr: 1.0000e-04\n",
      "Epoch 34/40\n",
      "18/18 [==============================] - 94s 5s/step - loss: 0.8262 - mae: 1.2330 - val_loss: 1.0602 - val_mae: 1.4464 - lr: 1.0000e-04\n",
      "Epoch 35/40\n",
      "18/18 [==============================] - 93s 5s/step - loss: 0.9225 - mae: 1.3412 - val_loss: 2.6647 - val_mae: 3.0805 - lr: 1.0000e-04\n",
      "Epoch 36/40\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.9347 - mae: 1.3580 - val_loss: 1.4563 - val_mae: 1.8869 - lr: 1.0000e-04\n",
      "Epoch 37/40\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.8632 - mae: 1.2757 - val_loss: 0.9568 - val_mae: 1.3744 - lr: 5.0000e-05\n",
      "Epoch 38/40\n",
      "18/18 [==============================] - 91s 5s/step - loss: 0.7457 - mae: 1.1463 - val_loss: 0.9951 - val_mae: 1.3971 - lr: 5.0000e-05\n",
      "Epoch 39/40\n",
      "18/18 [==============================] - 88s 5s/step - loss: 0.6744 - mae: 1.0716 - val_loss: 0.8550 - val_mae: 1.2305 - lr: 5.0000e-05\n",
      "Epoch 40/40\n",
      "18/18 [==============================] - 89s 5s/step - loss: 0.8160 - mae: 1.2262 - val_loss: 0.6768 - val_mae: 0.9999 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b73d76cf-5862-444e-ae3c-6d1ecb060bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 13s 3s/step - loss: 0.6768 - mae: 0.9999\n",
      "Validation Loss: 0.6768, Validation MAE: 0.9999 days\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Final Model on Validation Set\n",
    "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f} days\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "499ff1fa-ed15-4c37-b7ff-ea7694b0d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model \n",
    "model.load_weights(\"best_banana_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9faca8-1b24-4243-875f-7258444c9314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "\n",
      "Predicted Shelf Life: 8.01 days\n",
      "Recommended Sustainability Action: Distribute to fruit vendors or nearby markets\n"
     ]
    }
   ],
   "source": [
    "# Testing on a sample image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "img_path = r'C:\\Users\\Chhama\\OneDrive\\Desktop\\Banana\\Banana(5-10)\\frame270.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "img_array = img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "predicted_shelf_life = model.predict(img_array)[0][0]\n",
    "print(f\"\\nPredicted Shelf Life: {predicted_shelf_life:.2f} days\")\n",
    "\n",
    "# Sustainable distribution logic\n",
    "def shelf_life_action(days):\n",
    "    if days > 10:\n",
    "        return \"Store in warehouse or export to far locations\"\n",
    "    elif days > 5:\n",
    "        return \"Distribute to fruit vendors or nearby markets\"\n",
    "    else:\n",
    "        return \"Send to processing units for juices or quick-sale outlets\"\n",
    "\n",
    "action = shelf_life_action(predicted_shelf_life)\n",
    "print(f\"Recommended Sustainability Action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a430b-da72-4393-848e-38ad97abd523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
